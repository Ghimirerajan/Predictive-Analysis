{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea8d6be-d8ff-40e5-aa15-489b396bcd89",
   "metadata": {},
   "source": [
    "## Multi_Class Prediction of ObesityRisk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15726b63-71b7-4962-814c-3cb1ed41900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multinomial Logistic Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9052640-72e0-4dbc-890b-e6c3f8dd914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7f01fa-0dad-4aff-a63a-4417e6d379c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/ghimi/OneDrive/Desktop/PHD in Data Science/4_Predictive Analysis_TIM8555/Assignment_5/train.csv\")\n",
    "test = pd.read_csv(\"C:/Users/ghimi/OneDrive/Desktop/PHD in Data Science/4_Predictive Analysis_TIM8555/Assignment_5/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa5d1130-b4ea-4049-98e9-e421bbe3878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Sample:\n",
      "    id  Gender        Age    Height      Weight family_history_with_overweight  \\\n",
      "0   0    Male  24.443011  1.699998   81.669950                            yes   \n",
      "1   1  Female  18.000000  1.560000   57.000000                            yes   \n",
      "2   2  Female  18.000000  1.711460   50.165754                            yes   \n",
      "3   3  Female  20.952737  1.710730  131.274851                            yes   \n",
      "4   4    Male  31.641081  1.914186   93.798055                            yes   \n",
      "\n",
      "  FAVC      FCVC       NCP        CAEC SMOKE      CH2O SCC       FAF  \\\n",
      "0  yes  2.000000  2.983297   Sometimes    no  2.763573  no  0.000000   \n",
      "1  yes  2.000000  3.000000  Frequently    no  2.000000  no  1.000000   \n",
      "2  yes  1.880534  1.411685   Sometimes    no  1.910378  no  0.866045   \n",
      "3  yes  3.000000  3.000000   Sometimes    no  1.674061  no  1.467863   \n",
      "4  yes  2.679664  1.971472   Sometimes    no  1.979848  no  1.967973   \n",
      "\n",
      "        TUE       CALC                 MTRANS           NObeyesdad  \n",
      "0  0.976473  Sometimes  Public_Transportation  Overweight_Level_II  \n",
      "1  1.000000         no             Automobile        Normal_Weight  \n",
      "2  1.673584         no  Public_Transportation  Insufficient_Weight  \n",
      "3  0.780199  Sometimes  Public_Transportation     Obesity_Type_III  \n",
      "4  0.931721  Sometimes  Public_Transportation  Overweight_Level_II  \n",
      "Test Dataset Sample:\n",
      "       id  Gender        Age    Height      Weight  \\\n",
      "0  20758    Male  26.899886  1.848294  120.644178   \n",
      "1  20759  Female  21.000000  1.600000   66.000000   \n",
      "2  20760  Female  26.000000  1.643355  111.600553   \n",
      "3  20761    Male  20.979254  1.553127  103.669116   \n",
      "4  20762  Female  26.000000  1.627396  104.835346   \n",
      "\n",
      "  family_history_with_overweight FAVC      FCVC       NCP       CAEC SMOKE  \\\n",
      "0                            yes  yes  2.938616  3.000000  Sometimes    no   \n",
      "1                            yes  yes  2.000000  1.000000  Sometimes    no   \n",
      "2                            yes  yes  3.000000  3.000000  Sometimes    no   \n",
      "3                            yes  yes  2.000000  2.977909  Sometimes    no   \n",
      "4                            yes  yes  3.000000  3.000000  Sometimes    no   \n",
      "\n",
      "       CH2O SCC       FAF       TUE       CALC                 MTRANS  \n",
      "0  2.825629  no  0.855400  0.000000  Sometimes  Public_Transportation  \n",
      "1  3.000000  no  1.000000  0.000000  Sometimes  Public_Transportation  \n",
      "2  2.621877  no  0.000000  0.250502  Sometimes  Public_Transportation  \n",
      "3  2.786417  no  0.094851  0.000000  Sometimes  Public_Transportation  \n",
      "4  2.653531  no  0.000000  0.741069  Sometimes  Public_Transportation  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the datasets\n",
    "print(\"Train Dataset Sample:\\n\", train.head())\n",
    "print(\"Test Dataset Sample:\\n\", test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00e87e-5ad3-4751-862c-6767592abb5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f67032e-173d-4121-818b-e3db23cbc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Encoding categorical variables\n",
    "# Encode categorical columns like 'Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS', and 'NObeyesdad'\n",
    "le = LabelEncoder()\n",
    "for column in ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS', 'NObeyesdad']:\n",
    "    train[column] = le.fit_transform(train[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0f52e-214d-46d9-b369-96a5def9d55e",
   "metadata": {},
   "source": [
    "The code we provided uses LabelEncoder from the sklearn.preprocessing module to convert categorical variables into numeric values. The categorical columns listed in the train dataset (e.g., Gender, family_history_with_overweight, FAVC, etc.) are being encoded so that machine learning algorithms can work with the dataset. LabelEncoder(): This transforms categorical values into numerical ones (for example, encoding \"Male\" and \"Female\" as 0 and 1). for loop: It iterates over the columns that contain categorical data and applies LabelEncoder to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "037d9d62-c00e-4ffb-8e1f-10be040f45d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.443011</td>\n",
       "      <td>1.699998</td>\n",
       "      <td>81.669950</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.983297</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.763573</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976473</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.711460</td>\n",
       "      <td>50.165754</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.880534</td>\n",
       "      <td>1.411685</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.910378</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866045</td>\n",
       "      <td>1.673584</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20.952737</td>\n",
       "      <td>1.710730</td>\n",
       "      <td>131.274851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.674061</td>\n",
       "      <td>0</td>\n",
       "      <td>1.467863</td>\n",
       "      <td>0.780199</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31.641081</td>\n",
       "      <td>1.914186</td>\n",
       "      <td>93.798055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.679664</td>\n",
       "      <td>1.971472</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.979848</td>\n",
       "      <td>0</td>\n",
       "      <td>1.967973</td>\n",
       "      <td>0.931721</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Gender        Age    Height      Weight  \\\n",
       "0   0       1  24.443011  1.699998   81.669950   \n",
       "1   1       0  18.000000  1.560000   57.000000   \n",
       "2   2       0  18.000000  1.711460   50.165754   \n",
       "3   3       0  20.952737  1.710730  131.274851   \n",
       "4   4       1  31.641081  1.914186   93.798055   \n",
       "\n",
       "   family_history_with_overweight  FAVC      FCVC       NCP  CAEC  SMOKE  \\\n",
       "0                               1     1  2.000000  2.983297     2      0   \n",
       "1                               1     1  2.000000  3.000000     1      0   \n",
       "2                               1     1  1.880534  1.411685     2      0   \n",
       "3                               1     1  3.000000  3.000000     2      0   \n",
       "4                               1     1  2.679664  1.971472     2      0   \n",
       "\n",
       "       CH2O  SCC       FAF       TUE  CALC  MTRANS  NObeyesdad  \n",
       "0  2.763573    0  0.000000  0.976473     1       3           6  \n",
       "1  2.000000    0  1.000000  1.000000     2       0           1  \n",
       "2  1.910378    0  0.866045  1.673584     2       3           0  \n",
       "3  1.674061    0  1.467863  0.780199     1       3           4  \n",
       "4  1.979848    0  1.967973  0.931721     1       3           6  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first few rows to verify the encoding\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cd2ee86-4985-4ad6-ba1a-3c17a6fe2627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting features and target variable\n",
    "X_train = train.drop(columns=['id', 'NObeyesdad'])  # Features\n",
    "y_train = train['NObeyesdad']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57393b51-499a-4d63-b198-94186ea0573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']] = scaler.fit_transform(X_train[['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb0dac36-dcde-40b9-9ab9-232d6438cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cross-validation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b8aff53-688e-42a3-9dd2-de2d53ef8c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multinomial Logistic Regression:\n",
      "Mean Accuracy: 0.860 (0.006)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91      2523\n",
      "           1       0.85      0.81      0.83      3082\n",
      "           2       0.82      0.83      0.83      2910\n",
      "           3       0.93      0.96      0.95      3248\n",
      "           4       1.00      1.00      1.00      4046\n",
      "           5       0.72      0.69      0.71      2427\n",
      "           6       0.71      0.69      0.70      2522\n",
      "\n",
      "    accuracy                           0.86     20758\n",
      "   macro avg       0.85      0.85      0.85     20758\n",
      "weighted avg       0.86      0.86      0.86     20758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. Multinomial Logistic Regression\n",
    "print(\"\\nMultinomial Logistic Regression:\")\n",
    "log_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "log_scores = cross_val_score(log_model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(log_scores), np.std(log_scores)))\n",
    "log_model.fit(X_train, y_train)\n",
    "log_predictions = log_model.predict(X_train)\n",
    "print(classification_report(y_train, log_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172cfc3-351e-43db-8284-3e8b7f63a3d6",
   "metadata": {},
   "source": [
    "A logistic regression model is trained using the LogisticRegression() function from Scikit-learn with multi_class='multinomial' and the lbfgs solver.The cross_val_score function is used to calculate the accuracy over multiple cross-validation splits, showing an average accuracy of 0.860 with a standard deviation of 0.006.Classification Report: The model performance is reported for each class using metrics like precision, recall, and f1-score, along with the support (number of true instances for each label in the dataset):\n",
    "Class 0: Precision: 0.88, Recall: 0.94, F1-Score: 0.91 (2523 samples)\n",
    "Class 1: Precision: 0.85, Recall: 0.81, F1-Score: 0.83 (3082 samples)\n",
    "Class 2: Precision: 0.82, Recall: 0.83, F1-Score: 0.83 (2910 samples)\n",
    "Class 3: Precision: 0.93, Recall: 0.96, F1-Score: 0.94 (3248 samples)\n",
    "Class 4: Precision: 1.00, Recall: 1.00, F1-Score: 1.00 (4046 samples)\n",
    "Class 5: Precision: 0.72, Recall: 0.69, F1-Score: 0.71 (2427 samples)\n",
    "Class 6: Precision: 0.71, Recall: 0.69, F1-Score: 0.70 (2522 samples)\n",
    "\n",
    "Accuracy: 0.86 (which matches the mean accuracy from the cross-validation scores).Macro Average: Precision: 0.85, Recall: 0.85, F1-Score: 0.85. Weighted Average: Precision: 0.86, Recall: 0.86, F1-Score: 0.86. The model performs well overall, with high precision and recall for most classes, especially for class 4, which has perfect precision and recall. However, classes 5 and 6 have lower performance, indicating some difficulty in correctly classifying those categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a76ad32a-df8c-488f-9219-6e8211036dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Discriminant Analysis:\n",
      "Mean Accuracy: 0.819 (0.006)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87      2523\n",
      "           1       0.78      0.71      0.74      3082\n",
      "           2       0.81      0.77      0.79      2910\n",
      "           3       0.91      0.96      0.93      3248\n",
      "           4       0.99      1.00      0.99      4046\n",
      "           5       0.63      0.63      0.63      2427\n",
      "           6       0.65      0.65      0.65      2522\n",
      "\n",
      "    accuracy                           0.82     20758\n",
      "   macro avg       0.80      0.80      0.80     20758\n",
      "weighted avg       0.82      0.82      0.82     20758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Linear Discriminant Analysis\n",
    "print(\"\\nLinear Discriminant Analysis:\")\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_scores = cross_val_score(lda_model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(lda_scores), np.std(lda_scores)))\n",
    "lda_model.fit(X_train, y_train)\n",
    "lda_predictions = lda_model.predict(X_train)\n",
    "print(classification_report(y_train, lda_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c19989-133e-49be-ab17-936cd7939911",
   "metadata": {},
   "source": [
    "These results of Linear Discriminant Analysis (LDA) performed on the same classification problem, along with the associated classification report.Linear Discriminant Analysis (LDA) is The model is trained using LinearDiscriminantAnalysis() from Scikit-learn.\n",
    "The cross-validation scores (accuracy) have been computed, yielding a mean accuracy of 0.819 with a standard deviation of 0.006.Classification Report: The report provides metrics such as precision, recall, and f1-score for each class, along with the support (the number of true samples for each class in the dataset):\n",
    "\n",
    "Class 0: Precision: 0.83, Recall: 0.91, F1-Score: 0.87 (2523 samples)\n",
    "Class 1: Precision: 0.78, Recall: 0.71, F1-Score: 0.74 (3082 samples)\n",
    "Class 2: Precision: 0.81, Recall: 0.77, F1-Score: 0.79 (2910 samples)\n",
    "Class 3: Precision: 0.91, Recall: 0.96, F1-Score: 0.93 (3248 samples)\n",
    "Class 4: Precision: 0.99, Recall: 1.00, F1-Score: 0.99 (4046 samples)\n",
    "Class 5: Precision: 0.63, Recall: 0.63, F1-Score: 0.63 (2427 samples)\n",
    "Class 6: Precision: 0.65, Recall: 0.65, F1-Score: 0.65 (2522 samples)\n",
    "Macro Average: Precision: 0.80, Recall: 0.80, F1-Score: 0.80. Weighted Average: Precision: 0.82, Recall: 0.82, F1-Score: 0.82.\n",
    "\n",
    "LDA Mean Accuracy (0.819) is lower than the Multinomial Logistic Regression Mean Accuracy (0.860). The performance for classes 5 and 6 is also lower for LDA compared to the multinomial logistic regression model.\n",
    "\n",
    "However, LDA is still performing reasonably well with balanced precision and recall across most of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e46a4df1-f775-4016-acc8-8db2c4ab5441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quadratic Discriminant Analysis:\n",
      "Mean Accuracy: 0.195 (0.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:960: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2523\n",
      "           1       0.00      0.00      0.00      3082\n",
      "           2       0.00      0.00      0.00      2910\n",
      "           3       0.00      0.00      0.00      3248\n",
      "           4       0.19      1.00      0.33      4046\n",
      "           5       0.00      0.00      0.00      2427\n",
      "           6       0.00      0.00      0.00      2522\n",
      "\n",
      "    accuracy                           0.19     20758\n",
      "   macro avg       0.03      0.14      0.05     20758\n",
      "weighted avg       0.04      0.19      0.06     20758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 3. Quadratic Discriminant Analysis\n",
    "print(\"\\nQuadratic Discriminant Analysis:\")\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_scores = cross_val_score(qda_model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(qda_scores), np.std(qda_scores)))\n",
    "qda_model.fit(X_train, y_train)\n",
    "qda_predictions = qda_model.predict(X_train)\n",
    "print(classification_report(y_train, qda_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56906448-c6d1-458b-8aa9-cf5a49507c32",
   "metadata": {},
   "source": [
    "Several warnings and errors from Scikit-learn are presented, indicating multicollinearity among the variables in the dataset. Specifically, the message warnings.warn(\"Variables are collinear\") points to high collinearity between some of the features, which is problematic for QDA since it assumes that the predictor variables are normally distributed with different covariance matrices for each class.\n",
    "\n",
    "The mean accuracy is extremely low: 0.195 with a standard deviation of 0.000, indicating that the model is almost entirely misclassifying the data.\n",
    "For several classes (0, 1, 2, 3), the precision, recall, and F1-score are all 0.00, meaning the model is unable to correctly classify these classes.\n",
    "Only class 4 shows some performance with precision and recall at 0.19 and 1.00, respectively, which indicates that while the model always predicts class 4 when that class is present, it struggles significantly for other classes.\n",
    "The additional errors in the execution trace, involving matrix operations, point to the numerical instability caused by collinearity and perhaps even singular covariance matrices (matrices that cannot be inverted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f01d0-c0c0-42ba-94d4-17c6734ba085",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets Address Multicollinearity:\n",
    "lets  removing highly correlated features using techniques like Variance Inflation Factor (VIF) or simply by dropping some features after assessing their correlation matrix.\n",
    "we  can also apply Principal Component Analysis (PCA) or other dimensionality reduction techniques to reduce collinearity before running QDA.\n",
    "\n",
    "Ensure that our features are standardized (mean=0, variance=1), as QDA is sensitive to the scale of input variables.\n",
    "Given the issues with QDA, consider using other classification models like Ridge Regression, SVM, or even sticking with Multinomial Logistic Regression, which performed better earlier in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2da4f-e170-42a1-b82c-f0532ab426e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9184f81-4faf-4938-8ca2-e7b71a7cee39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naïve Bayes:\n",
      "Mean Accuracy: 0.644 (0.007)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.56      0.55      2523\n",
      "           1       0.46      0.59      0.52      3082\n",
      "           2       0.60      0.48      0.53      2910\n",
      "           3       0.79      0.92      0.85      3248\n",
      "           4       0.99      1.00      1.00      4046\n",
      "           5       0.48      0.29      0.36      2427\n",
      "           6       0.41      0.42      0.42      2522\n",
      "\n",
      "    accuracy                           0.65     20758\n",
      "   macro avg       0.61      0.61      0.60     20758\n",
      "weighted avg       0.64      0.65      0.64     20758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Naïve Bayes\n",
    "print(\"\\nNaïve Bayes:\")\n",
    "nb_model = BernoulliNB()\n",
    "nb_scores = cross_val_score(nb_model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(nb_scores), np.std(nb_scores)))\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_predictions = nb_model.predict(X_train)\n",
    "print(classification_report(y_train, nb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af847c-ff3b-4f67-9487-1cdfe3bb3481",
   "metadata": {},
   "source": [
    "This is a Bernoulli Naive Bayes model) along with the classification report. Here's a breakdown of the results.A BernoulliNB model has been trained, and the cross-validation scores (accuracy) are computed. The model yields a mean accuracy of 0.644 with a standard deviation of 0.007.\n",
    "The precision, recall, and F1-score are reported for each class, along with the support (number of instances for each class in the dataset):\n",
    "\n",
    "Class 0: Precision: 0.54, Recall: 0.56, F1-Score: 0.55 (2523 samples)\n",
    "\n",
    "Class 1: Precision: 0.46, Recall: 0.59, F1-Score: 0.52 (3082 samples)\n",
    "\n",
    "Class 2: Precision: 0.60, Recall: 0.48, F1-Score: 0.54 (2910 samples)\n",
    "\n",
    "Class 3: Precision: 0.79, Recall: 0.92, F1-Score: 0.85 (3248 samples)\n",
    "\n",
    "Class 4: Precision: 0.99, Recall: 1.00, F1-Score: 1.00 (4046 samples)\n",
    "\n",
    "Class 5: Precision: 0.48, Recall: 0.29, F1-Score: 0.36 (2427 samples)\n",
    "\n",
    "Class 6: Precision: 0.41, Recall: 0.42, F1-Score: 0.42 (2522 samples)\n",
    "\n",
    "Accuracy: 0.65, which is lower than the previous models you’ve tried (multinomial logistic regression, linear discriminant analysis).\n",
    "Macro Average: Precision: 0.61, Recall:\n",
    "Weighted Average: Precision: 0.64, Recall: 0.65, F1-Score: 0.64.\n",
    "\n",
    "The performance of Naive Bayes is not as strong as the multinomial logistic regression or LDA models. This model seems to struggle with precision and recall for most classes, especially classes 1, 2, 5, and 6.\n",
    "Class 4 again performs very well, with nearly perfect precision, recall, and F1-score, similar to other models you've tried.\n",
    "The lower precision and recall for classes 1, 2, 5, and 6 suggest that Naive Bayes is having difficulty distinguishing between some of these categories, possibly due to the feature distribution assumptions of the BernoulliNB model.\n",
    "\n",
    "Next \n",
    "The Bernoulli Naive Bayes model works best when features are binary. If  features aren't binary, you might want to binarize or transform them to better suit this model.\n",
    "If the features are continuous, you might have more success with GaussianNB instead of BernoulliNB, as it handles continuous data better.\n",
    "Since multinomial logistic regression performed better overall, you may want to focus on tuning that model or try other classifiers like Random Forest or SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15344a6e-d0fe-455a-9d68-d47390d5d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support Vector Machine:\n",
      "Mean Accuracy: 0.747 (0.007)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghimi\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89      2523\n",
      "           1       0.63      0.65      0.64      3082\n",
      "           2       0.59      0.68      0.63      2910\n",
      "           3       0.86      0.97      0.91      3248\n",
      "           4       0.98      1.00      0.99      4046\n",
      "           5       0.54      0.40      0.46      2427\n",
      "           6       0.55      0.39      0.45      2522\n",
      "\n",
      "    accuracy                           0.75     20758\n",
      "   macro avg       0.71      0.72      0.71     20758\n",
      "weighted avg       0.73      0.75      0.74     20758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Support Vector Machine\n",
    "print(\"\\nSupport Vector Machine:\")\n",
    "svc_model = LinearSVC(max_iter=10000)\n",
    "svc_scores = cross_val_score(svc_model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(svc_scores), np.std(svc_scores)))\n",
    "svc_model.fit(X_train, y_train)\n",
    "svc_predictions = svc_model.predict(X_train)\n",
    "print(classification_report(y_train, svc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421fdee-a0c6-4c62-9a27-5297726d4e4d",
   "metadata": {},
   "source": [
    "The SVM model has been trained, and the cross-validation scores (accuracy) yield a mean accuracy of 0.747 with a standard deviation of 0.007.\n",
    "The precision, recall, and F1-score for each class are presented, along with the support (number of instances for each class):\n",
    "Class 0: Precision: 0.83, Recall: 0.96, F1-Score: 0.89 (2523 samples)\n",
    "Class 1: Precision: 0.63, Recall: 0.65, F1-Score: 0.64 (3082 samples)\n",
    "Class 2: Precision: 0.59, Recall: 0.68, F1-Score: 0.63 (2910 samples)\n",
    "Class 3: Precision: 0.86, Recall: 0.97, F1-Score: 0.91 (3248 samples)\n",
    "Class 4: Precision: 0.98, Recall: 1.00, F1-Score: 0.99 (4046 samples)\n",
    "Class 5: Precision: 0.54, Recall: 0.40, F1-Score: 0.46 (2427 samples)\n",
    "Class 6: Precision: 0.55, Recall: 0.39, F1-Score: 0.45 (2522 samples)\n",
    "\n",
    "Accuracy: 0.75, which is a modest improvement over the Naive Bayes model but still lower than the Multinomial Logistic Regression.\n",
    "Macro Average: Precision: 0.71, Recall: 0.72, F1-Score: 0.71.\n",
    "Weighted Average: Precision: 0.73, Recall: 0.75, F1-Score: 0.74.\n",
    "Class 4 once again performs exceptionally well with almost perfect precision, recall, and F1-score.\n",
    "Classes like 5 and 6 still show lower performance, with F1-scores of 0.46 and 0.45 respectively.\n",
    "Overall, the SVM model performs reasonably well, especially for the larger classes like 0, 3, and 4. However, it struggles with precision and recall for the smaller classes (5 and 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338874b-0360-4003-9cac-41dff95f879e",
   "metadata": {},
   "source": [
    "Consider tuning the SVM’s hyperparameters (e.g., C, kernel, gamma) using techniques like GridSearchCV or RandomizedSearchCV to improve performance.\n",
    "Classes 5 and 6 appear to be more difficult to classify. You might want to explore class weighting or oversampling techniques (like SMOTE) to give more attention to these minority classes.\n",
    "The Multinomial Logistic Regression and Linear Discriminant Analysis have shown stronger performance in earlier results. You could explore additional feature engineering or ensemble methods to improve further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd86be-aa10-4448-9859-8a05696207f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
